{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved encoder model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import Lang, ToxicityDataset, collate, normalizeString\n",
    "\n",
    "lang = Lang(\"eng\")\n",
    "data = pd.read_csv('data/train_2024.csv', quoting = 3)\n",
    "df = pd.DataFrame(data)\n",
    "for sentence in df['text']:\n",
    "    lang.addSentence(normalizeString(sentence))   \n",
    "\n",
    "trainset = ToxicityDataset('data/train_2024.csv', 'id', 'text', 'label', lang)\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('data/dev_2024.csv', quoting = 3)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "valset = ToxicityDataset('data/dev_2024.csv', 'id', 'text', 'label', lang)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test_data.csv', quoting = 3)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "#for sentence in test_df['text']:\n",
    "#    lang.addSentence(normalizeString(sentence))\n",
    "\n",
    "testset = ToxicityDataset('./test_data.csv', 'id', 'text', 'label', lang)\n",
    "test_loader = DataLoader(testset, batch_size=1, shuffle=False, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the saved encoder model\n",
    "from encoder import EncoderClassifier, Encoder, MLPClassifier\n",
    "\n",
    "embed_size = 256\n",
    "bert_encoder = Encoder(src_vocab_size=trainset.lang.n_words, n_blocks = 3, n_features = embed_size, n_heads = 4, n_hidden = 512, dropout = 0.1, max_length = 5000)\n",
    "classifier = MLPClassifier(n_features = embed_size, num_classes = 2, num_layers = 2, dropout = 0.1)\n",
    "model = EncoderClassifier(bert_encoder, classifier)\n",
    "model.load_state_dict(torch.load('encoder.pth', map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_loader):\n",
    "    model.eval()\n",
    "    accuracy = []\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, mask, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(torch.float32).reshape(labels.size(0), 1).to(device)\n",
    "\n",
    "        outputs = model(inputs, mask)\n",
    "\n",
    "        out = torch.round(torch.sigmoid(outputs))\n",
    "        accuracy.append(torch.sum(out == labels).item() / labels.size(0))\n",
    "        print(i)\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "    accuracy = np.mean(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "Validation accuracy:  0.8046875\n"
     ]
    }
   ],
   "source": [
    "print('Validation accuracy: ', compute_accuracy(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,   66,   25, 4354, 2264,  413,   25,  122,   28,   25,   28,  413,\n",
      "          102, 2402,   13,   15, 1780,  153,  882,   34, 1224,   28,    1]])\n",
      "0\n",
      "tensor([[   0, 1049,   59,  162,  153,   58, 4110,  108,   15, 1060,  548,  343,\n",
      "           25,  482,   15, 1879, 2287,  207, 3190, 4900, 1274,  108,  413,  236,\n",
      "         1061,  102,  145,  321,  335,  149,   48, 2364,   25,  336,   24,    3,\n",
      "            1]])\n",
      "tensor([[    0,    25,   848,  4235,    58,    18,    68,    13,    25,   413,\n",
      "           320,   413,   273,    42,    48,  1303,   554,    25,  2379,   413,\n",
      "            15,    18,   945,  7626,  1536,     3,  1808,   162,   449,   413,\n",
      "            25,  1793,  2436,    25,  1087,   318,    35,  1330,    49,  1162,\n",
      "            25,  1793,   111,    25,  1087,   318,    49,   197,    48,   413,\n",
      "           615,   145, 31432, 66431,  1692,    28,    25,   855,    15,    25,\n",
      "         21686,    75,     1]])\n",
      "tensor([[    0,   413,   113,    65,   247,    49,   375,   145,   200,    48,\n",
      "            25,  1567,   359,   127,   109,    13,     3, 20662,  5901,   413,\n",
      "           642,   109,   413,  3114,  2567,   413,   146,    61,   359, 21328,\n",
      "           413,  3124,  2049,    15,  2647,   413,    38,   642,    49,    34,\n",
      "          3053,    52,   105,  1302,   413,    38,  7362,   459,  6775,   413,\n",
      "           247,   207,   127,   173,    15,  2004,   113,   413,    34,   224,\n",
      "           413,  1216,   153,  1554,    45,  7136,   375,   413,    44,   413,\n",
      "          1007,     1]])\n",
      "tensor([[   0, 2072,  717, 3555,  467,   28,   25, 2186, 6122,   23,  404,   25,\n",
      "         2072,    3,  717,  915,  467, 2186,  413,  413, 1150,  627, 2900, 2649,\n",
      "            3,   17,  109,  666, 4696, 3555,  467,  141,  413,  153,  141,   18,\n",
      "          413,  744,  415,   65,  744,  262,   15,   65,  127,   48,  128,   48,\n",
      "           66,  117,   28,   25,  413,  889,  104,    3, 1094,  186,   65,  248,\n",
      "         1577,   49,   15,  295,   49,  170,  413,    1]])\n",
      "tensor([[   0,  111,  226, 2401,  145, 9592,  108,   25,  635, 2401,  529,  109,\n",
      "          145, 1848,   48, 1131, 3684,  413, 2401,  127, 6590,    3,  145,  615,\n",
      "         1862,  226, 2401,  216,   25,  615,  654, 2050,    1]])\n",
      "tensor([[    0,   413,    38,   437,    49,  2066,    48,  5712,  2029,    24,\n",
      "          1173,   413,    15,    84,   321,    65,  5712,    49,    49,   248,\n",
      "           330,   145,  1487,    34,   369,   111, 30418,    25,  6044,    15,\n",
      "           827,    25, 26037,  1066,    49,   248,   145,    34,  2975,  4254,\n",
      "            50,  2655,  3413,   529,   224,   145,  8465,    48,  4772,    25,\n",
      "          2029,    34,     1]])\n",
      "tensor([[   0,  642,  207, 7301,  449, 1953,   48, 1001,   25,   84, 1956,  117,\n",
      "         2466,  413,  520,  101, 2888, 5712,  646,    1]])\n",
      "tensor([[   0,   67,  413,  113,   65, 7266, 1179,   25, 1497, 7474,  127, 2415,\n",
      "          148, 1284,  310,  413,  404,   18, 2621,   28,  310,  434, 5034,   34,\n",
      "           41,  488,   13, 5119,  413,   65, 1798,  413,   18,    1]])\n",
      "tensor([[    0,   413,    15,   413,    24,   367, 24132,   383,    18,     1]])\n",
      "tensor([[    0,  7331,  3225,   413,   109,    25,  1251,   413,   548,    65,\n",
      "           912,   413,    24,   655,   413,    76,   429,  1459,   153,   224,\n",
      "           475,   168,    77,   413,   207,  4138,   170,    42,   551, 42478,\n",
      "            18,   667,    18, 10890,    25,  1161,   413,   313,   413,   109,\n",
      "           111,  1534,  6026,    18,   413,   413,    20,   335,   642, 13241,\n",
      "            67,   945,    48,  3398,  8381, 18266,   413,   413,   413,    85,\n",
      "            34,   413,    25,   257,    65,   111,   858,    25,  1376,   413,\n",
      "           548,   207,  1534,    48,   121,   207,   413,    10,    10,    10,\n",
      "             1]])\n",
      "tensor([[   0,  413, 1525, 9655,  216,   25, 3166,  413,   76,  145,  971, 2448,\n",
      "          413,  598, 8233,  170,   48,   25, 5510,   28,  213, 2773,  413, 1554,\n",
      "          413,  413,   24,  262,   18, 2108, 2263,  415,  246,  149, 1307,  114,\n",
      "           25, 2764,  123,    1]])\n",
      "tensor([[   0,  413,   25,  356,   28,   25, 1015,  529,  145,    1]])\n",
      "tensor([[    0,   208,    48, 13137,    25,   294,    48,  5764,     1]])\n",
      "tensor([[    0,  3693,   413,   504,   413,   413,    15,  1012,  4642,   413,\n",
      "           912,    25,   130,   413,  1899,   162, 11165, 50489,  4642,    28,\n",
      "           359,   413,   413,    15,  2579,   413,  1216,    49,    24,    48,\n",
      "          2523,    18, 11165,  5278,    28,   359,   383,   413,    13,    18,\n",
      "          2033,   102,   145, 14343,  7337,    28,    25,   246,  2021,     3,\n",
      "            49,    24,   109,   267, 17157,    48,  2666,    25,   168,    73,\n",
      "            28,   349,    83,   413,    76,  2021,    49,    24,   459,  6198,\n",
      "            48,   113,    24,    20,    25,   149,  1248,   162,   459,    38,\n",
      "            58,    25, 17132,    48,  3679,   383,    18,   413,   149,   666,\n",
      "          1967,   216,    25,    41,  2108,   140,   636,    12,  6376,   127,\n",
      "          3370,   108,   459,     1]])\n",
      "tensor([[    0,    50,   111,    48,    24,    18,   899,   965,   435,   413,\n",
      "            34,    41,  4620,    65,  2666,   186,    65,   248,    18, 11797,\n",
      "           535,    15,   579,    69,    24,    18, 45479,   109,    50,    28,\n",
      "            25,  1760,    76,    41,  2438,  1093,   108,     1]])\n",
      "tensor([[    0,   413,    25,   463,  6659,   490, 22092,   870,   413,    15,\n",
      "            18,   413,  7872,  3638,   413,   413,   413,   273,   164,   145,\n",
      "            18,   413,   490,  2502,   111,    65,   413,   413,   556,     1]])\n",
      "tensor([[0, 1]])\n",
      "tensor([[    0,   117,   108,    25, 25993,   413,   658,    13,    18,   168,\n",
      "            18,    65,    15,    67,   557,   145, 14008,   321,    65,    50,\n",
      "          1534,    18, 23378,     1]])\n",
      "tensor([[   0,  248,   65,  104,    3,  207,  127,  146,   19,  162,  543,   65,\n",
      "          274,   48,  299,  349, 3203,    1]])\n",
      "tensor([[    0,   102,   106,   430, 12129,    13, 17850,   413,   717,   149,\n",
      "            25,   229,    28,  4191, 11299,   227,    62,   413,   717,   145,\n",
      "          1518,    48, 11338,    34,    25, 21287,    28,  1016,    42,    13,\n",
      "           359,  2670,    24,    25, 23483, 10542,     3,   102,   145,  5552,\n",
      "            13,  2874,   413,    24,    18,  2336,  1392,    28,   551,  2134,\n",
      "          1151,   162,   678,   413,    15,   213,  3244,   687,    13,  1016,\n",
      "            24,   105,   475,    34,  1016,    48,  1302,   413,   267,    48,\n",
      "           145,   646,  5230,  1016,     1]])\n",
      "tensor([[    0, 51568,   127,    65,  1953,   330,   117,    12,  1676,  4818,\n",
      "            15,    12,    85,   108,   141,   226,   416,  1165,   162,    25,\n",
      "            58,  3009,     1]])\n",
      "tensor([[    0,    70,    25,  4556,    24,    52,    25,   647,    28,    18,\n",
      "         20129,  1440,  5384,    34,  2582, 11291,   184,    25,   413,   224,\n",
      "            28,    25,  2582,   935,   566,   262,   127,  4825,    48,  1446,\n",
      "           184,  1863,    48,  4998,    25,  4718, 31846,   413,    15,  4287,\n",
      "           184,    18,   447,   127,   132,  2740,   162,    25,    28,     1]])\n",
      "tensor([[    0,  4325,   924,   491,    15,  1879,    13,  5005,   413,    15,\n",
      "           349,   281, 13235,    12,   916, 11067,   413,     1]])\n",
      "tensor([[   0, 1202, 6122,   18,  413,  340,    1]])\n",
      "tensor([[   0, 9402, 2242,  260,    1]])\n",
      "tensor([[    0, 17505,    74,   615,  4268,  1016,   186,   226,  3915,   111,\n",
      "          1016,   102,  1849,    13,   459,   413,     1]])\n",
      "tensor([[   0,  413,   76,   48,  318,   15, 2032,  227,    1]])\n",
      "tensor([[    0,    20,    65,   274,   227,    25,   413,    65,   577,    24,\n",
      "             3,    65,   275,    18,    13,    25,  1162,  7452,   551,   246,\n",
      "            24,    13,   932,   162,   413,   413,  6111,   413,    15,  3838,\n",
      "          1534,    65,    67,    25, 45304,   413,    15,   320,  6300,   102,\n",
      "          2523,    67,  2265,    34,    20,   335,    18,  2410,    52,   147,\n",
      "           413,     1]])\n",
      "tensor([[    0,   102,    58,    48, 21835,   293,    28,  1554,    68,    34,\n",
      "            25,  2576,   413,    13,  2567,  7913,    15,  4128,    66,  1592,\n",
      "            28,  1127,  3458,    25,  5255,    13,   413,    25,   322,   383,\n",
      "            12,   581,    48,   153,   413,     1]])\n",
      "tensor([[   0, 1575, 1366,  413,   67, 1366,  497, 1575,   48,  551,  413,   49,\n",
      "          497, 1575,   48,    1]])\n",
      "tensor([[   0, 4713,   24, 1756,  879,   48,    1]])\n",
      "tensor([[   0,  117,   34,   73,  310,  127, 1488,   25, 2558,  367,   18,    1]])\n",
      "tensor([[    0,   413,   929,    28,    25,    28,    15,  5112, 30957,    34,\n",
      "            25,    28,   413,   413,   413,    10,   413,   413,    10,   413,\n",
      "           413,   413,    28,   413,   413,   105,   252,   413,    28,    24,\n",
      "           620,    25,    28,     1]])\n",
      "tensor([[    0,    83, 25760,    15,    38,   247,   349,  3542,   127,  2599,\n",
      "           246,   102,    68,    34,    13,    25,   524,  5515,   349,  9214,\n",
      "          5924,    15,  1219,   383,     1]])\n",
      "tensor([[    0, 10448,    48,   889,     3,   153,  1082,   149,  9230,    48,\n",
      "          1813,   413,  3984,    28,   221, 43562,    52,    25,   881,    12,\n",
      "           413,   227,    25,   668,  3402,    48,    25,     1]])\n",
      "tensor([[    0,    24,    67,  9406,   467,   542,  1102,    28,  1497,    15,\n",
      "            75,   149,   437,  5704,    48,  5086,  1629,    48,    58,   559,\n",
      "           162,  3203,    15,  3796,   108,    25,   135,   646,  1060,   413,\n",
      "           153,    24,   548,    65,  8446,    15, 13595, 12250,    48,  1158,\n",
      "           543,    15,   162,   664,  3449,    15,  4280,   413,    24,  3449,\n",
      "            15,   336,  2020,   117,    25,    31,    13,  2964,   103,    65,\n",
      "         17410,    25, 25768,  1150,   108,  4465,     1]])\n",
      "tensor([[    0,   413,   127,   173,   413,   383,   207,  1848,  2063,  5736,\n",
      "            48,  1244,   153,   483,  1951,   102,  8655,   321,   207,   580,\n",
      "           127,    50,    18, 14310,    28,  3458,    48,    25, 25921,   141,\n",
      "            25,   337,  1331,  5655,    15,     1]])\n",
      "tensor([[   0, 3166,   25, 1636,  479,    1]])\n",
      "tensor([[   0,  971, 1469,   65,  978,   13,   18,  964,  829,   28, 2483,  385,\n",
      "          252, 1779,  170,  413,  413,  226,  346,  108,   67, 9822,  135, 1221,\n",
      "           28,  490,  413,  413,  413,   66, 2965,  626,   65,  208,   49,    1]])\n",
      "tensor([[   0, 3914,  413,  226,  248,    1]])\n",
      "tensor([[   0,  152,  385, 3990,   28,   18,  413,    1]])\n",
      "tensor([[   0,   12,   18, 2030,   28, 1548,   48, 3628,   18, 2273, 1310,    1]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, mask)\n\u001b[1;32m     18\u001b[0m pre \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(outputs))\n\u001b[1;32m     19\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((predicted, pre), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Aalto/snlp/snlp_toxicity/encoder.py:111\u001b[0m, in \u001b[0;36mEncoderClassifier.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#Take the [CLS] token\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#x = x[:,   0, :]\u001b[39;00m\n\u001b[1;32m    109\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Aalto/snlp/snlp_toxicity/encoder.py:94\u001b[0m, in \u001b[0;36mMLPClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame(columns = ['id', 'label'])\n",
    "model.eval()\n",
    "\n",
    "predicted = torch.tensor([]).to(device)\n",
    "translations = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    inputs, mask, labels = data\n",
    "    \n",
    "    #translated = [testset.lang.index2word[i.item()] for input in inputs for i in input]\n",
    "    #translations.append(translated)\n",
    "    #print(translated)\n",
    "    print(inputs)\n",
    "    inputs = inputs.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    outputs = model(inputs, mask)\n",
    "\n",
    "    pre = torch.round(torch.sigmoid(outputs))\n",
    "    predicted = torch.cat((predicted, pre), dim = 0)\n",
    "    if i % 200 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12001, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = predicted.squeeze().detach().numpy().astype(int)\n",
    "predictions['label'] = predicted\n",
    "predictions['id'] = test_data['id']\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id  label\n",
       "0          0      1\n",
       "1          1      0\n",
       "2          2      0\n",
       "3          3      1\n",
       "4          4      1\n",
       "...      ...    ...\n",
       "11996  11996      1\n",
       "11997  11997      0\n",
       "11998  11998      1\n",
       "11999  11999      1\n",
       "12000  12000      0\n",
       "\n",
       "[12001 rows x 2 columns]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
