{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from loader import Lang, ToxicityDataset, normalizeString, collate\n",
    "\n",
    "lang = Lang(\"eng\")\n",
    "data = pd.read_csv('data/train_2024.csv')\n",
    "df = pd.DataFrame(data)\n",
    "for sentence in df['text']:\n",
    "    lang.addSentence(normalizeString(sentence))   \n",
    "\n",
    "trainset = ToxicityDataset('data/train_2024.csv', 'id', 'text', 'label', lang)\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:\n",
      " as word indices:  tensor([   50,    77, 15525,    10,   197,   929,   490,   144,     1])\n",
      " as string:  just another dreamer . go president trump ! EOS\n"
     ]
    }
   ],
   "source": [
    "#Get random sample from trainset\n",
    "src_seq, label = trainset[np.random.choice(len(trainset))]\n",
    "print('Source sentence:')\n",
    "print(' as word indices: ', src_seq)\n",
    "print(' as string: ', ' '.join(trainset.lang.index2word[i.item()] for i in src_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test_2024.csv')\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_lang = Lang(\"eng\")\n",
    "for sentence in test_df['text']:\n",
    "    test_lang.addSentence(normalizeString(sentence))\n",
    "\n",
    "testset = ToxicityDataset('data/test_2024.csv', 'id', 'text', 'label', test_lang)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset), shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"This implementation is the same as in the Annotated transformer blog post\n",
    "        See https://nlp.seas.harvard.edu/2018/04/03/attention.html for more detail.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=300):\n",
    "        assert (d_model % 2) == 0, 'd_model should be an even number.'\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden = 1024, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads, batch_first = True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x2, _ = self.attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(x2)\n",
    "        x = self.norm1(x)\n",
    "        x2 = self.feed_forward(x)\n",
    "        x = x + self.dropout2(x2)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clones(module, N):\n",
    "    \"Produces N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_blocks = 4, n_features = 256, n_heads = 16, n_hidden=64, dropout=0.1, max_length = 5000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, n_features)\n",
    "        self.pos_embedding = nn.Embedding(max_length, n_features)\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_features, n_heads, n_hidden, dropout) for _ in range(n_blocks)])\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        B, T = x.size()\n",
    "        positions = torch.arange(0, T, device = device)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_embedding(positions)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier on top of the encoder\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, n_features=512, num_classes=2, num_layers=3, dropout=0.2):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        #A single layer\n",
    "        layers = [\n",
    "            nn.Linear(n_features, n_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ]\n",
    "        \n",
    "        #Append all layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(n_features * 4, n_features * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "\n",
    "        #The output layer\n",
    "        if num_classes == 2:\n",
    "            layers.append(nn.Linear(n_features * 4, 1))\n",
    "        else:\n",
    "            layers.append(nn.Linear(n_features * 4, num_classes))\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder(x, mask)\n",
    "        \n",
    "        # Take the [CLS] token\n",
    "        # x = x[:, 0, :]\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # Apply sigmoid activation for binary classification\n",
    "        #x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SimpleEncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512):\n",
    "        super(SimpleEncoderClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "        # Final classification layer with a single output unit and sigmoid activation\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1),\n",
    "            nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        # Take the mean over the sequence dimension\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Pass through classifier\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "\n",
    "#This correponds to the first model I tried\n",
    "bert_encoder = Encoder(src_vocab_size=trainset.lang.n_words, n_blocks = 3, n_features = embed_size, n_heads = 4, n_hidden = 512, dropout = 0.1, max_length = 5000)\n",
    "simple_encoder = Encoder(src_vocab_size = trainset.lang.n_words, n_blocks = 2, n_features = embed_size, n_heads = 1, n_hidden = 64, dropout = 0.1, max_length = 5000)\n",
    "classifier = MLPClassifier(n_features = embed_size, num_classes = 2, num_layers = 2, dropout = 0.1)\n",
    "encoder_classifier = EncoderClassifier(bert_encoder, classifier)\n",
    "\n",
    "#This is a simpler one with much less parameters\n",
    "simple_encoder = Encoder(src_vocab_size = trainset.lang.n_words, n_blocks = 2, n_features = 128, n_heads = 4, n_hidden = 256, dropout = 0.2, max_length = 5000)\n",
    "simple_classifier = MLPClassifier(n_features = 128, num_classes = 2, num_layers = 1, dropout = 0.2)\n",
    "encoder_classifier_2 = EncoderClassifier(simple_encoder, simple_classifier)\n",
    "\n",
    "#This is the simplest one, which implements pytorch builtin encoder layers and a simple classifier in one class\n",
    "simple_encoder_classifier = SimpleEncoderClassifier(vocab_size = trainset.lang.n_words, embedding_dim=256)\n",
    "\n",
    "model = encoder_classifier\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 0.6912\n",
      "Epoch 1, Batch 2, Loss: 0.6711\n",
      "Epoch 1, Batch 3, Loss: 0.6639\n",
      "Epoch 1, Batch 4, Loss: 0.6926\n",
      "Epoch 1, Batch 5, Loss: 0.6951\n",
      "Epoch 1, Batch 6, Loss: 0.6252\n",
      "Epoch 1, Batch 7, Loss: 0.6692\n",
      "Epoch 1, Batch 8, Loss: 0.6505\n",
      "Epoch 1, Batch 9, Loss: 0.6526\n",
      "Epoch 1, Batch 10, Loss: 0.6505\n",
      "Epoch 1, Batch 11, Loss: 0.6494\n",
      "Epoch 1, Batch 12, Loss: 0.6284\n",
      "Epoch 1, Batch 13, Loss: 0.6833\n",
      "Epoch 1, Batch 14, Loss: 0.6047\n",
      "Epoch 1, Batch 15, Loss: 0.6588\n",
      "Epoch 1, Batch 16, Loss: 0.6419\n",
      "Epoch 1, Batch 17, Loss: 0.7426\n",
      "Epoch 1, Batch 18, Loss: 0.6420\n",
      "Epoch 1, Batch 19, Loss: 0.6493\n",
      "Epoch 1, Batch 20, Loss: 0.6497\n",
      "Epoch 1, Batch 21, Loss: 0.6311\n",
      "Epoch 1, Batch 22, Loss: 0.6345\n",
      "Epoch 1, Batch 23, Loss: 0.6279\n",
      "Epoch 1, Batch 24, Loss: 0.6449\n",
      "Epoch 1, Batch 25, Loss: 0.6535\n",
      "Epoch 1, Batch 26, Loss: 0.6041\n",
      "Epoch 1, Batch 27, Loss: 0.5375\n",
      "Epoch 1, Batch 28, Loss: 0.6911\n",
      "Epoch 1, Batch 29, Loss: 0.6813\n",
      "Epoch 1, Batch 30, Loss: 0.6892\n",
      "Epoch 1, Batch 31, Loss: 0.6862\n",
      "Epoch 1, Batch 32, Loss: 0.7066\n",
      "Epoch 1, Batch 33, Loss: 0.6867\n",
      "Epoch 1, Batch 34, Loss: 0.6946\n",
      "Epoch 1, Batch 35, Loss: 0.6613\n",
      "Epoch 1, Batch 36, Loss: 0.6658\n",
      "Epoch 1, Batch 37, Loss: 0.6554\n",
      "Epoch 1, Batch 38, Loss: 0.6733\n",
      "Epoch 1, Batch 39, Loss: 0.6570\n",
      "Epoch 1, Batch 40, Loss: 0.6378\n",
      "Epoch 1, Batch 41, Loss: 0.6438\n",
      "Epoch 1, Batch 42, Loss: 0.6553\n",
      "Epoch 1, Batch 43, Loss: 0.6746\n",
      "Epoch 1, Batch 44, Loss: 0.6026\n",
      "Epoch 1, Batch 45, Loss: 0.7130\n",
      "Epoch 1, Batch 46, Loss: 0.6463\n",
      "Epoch 1, Batch 47, Loss: 0.6616\n",
      "Epoch 1, Batch 48, Loss: 0.5579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 36\u001b[0m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Print the outputs, labels, and loss for debugging\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#print(f\"Outputs: {outputs}\")\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#print(f\"Labels: {labels}\")\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#print(f\"Loss: {loss}\")\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Compute total loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Syed Ashaf\\anaconda3\\envs\\snlp\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Syed Ashaf\\anaconda3\\envs\\snlp\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas = (0.9, 0.98), eps=1e-9)\n",
    "\n",
    "#Calculate the positive weight fraction\n",
    "positive  = sum([label for _, label in trainset])\n",
    "negative = len(trainset) - positive\n",
    "positive_weight = negative/positive\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(positive_weight).to(device))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, mask, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(torch.float32).to(device)\n",
    "        labels = labels.to(torch.float32).reshape(labels.size(0), 1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Print the outputs, labels, and loss for debugging\n",
    "        #print(f\"Outputs: {outputs}\")\n",
    "        #print(f\"Labels: {labels}\")\n",
    "        #print(f\"Loss: {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Print batch loss\n",
    "        print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Calculate epoch-level statistics\n",
    "epoch_loss = total_loss / len(train_loader)\n",
    "epoch_accuracy = correct / total * 100.0\n",
    "\n",
    "print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0464, Accuracy: 63.74%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, mask, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs, mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
