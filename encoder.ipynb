{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"This implementation is the same as in the Annotated transformer blog post\n",
    "        See https://nlp.seas.harvard.edu/2018/04/03/attention.html for more detail.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=300):\n",
    "        assert (d_model % 2) == 0, 'd_model should be an even number.'\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden = 64, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x2, _ = self.attn(x, x, x, key_padding_mask=mask.T)\n",
    "        x = x + self.dropout1(x2)\n",
    "        x = self.norm1(x)\n",
    "        x2 = self.feed_forward(x)\n",
    "        x = x + self.dropout2(x2)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clones(module, N):\n",
    "    \"Produces N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_blocks = 4, n_features = 256, n_heads = 16, n_hidden=64, dropout=0.1, max_length = 5000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, n_features)\n",
    "        #self.pos_embedding = nn.Embedding(max_length, n_features)\n",
    "        self.pos_encoder = PositionalEncoding(n_features, dropout, max_length)\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_features, n_heads, n_hidden, dropout) for _ in range(n_blocks)])\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder.forward(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier on top of the encoder\n",
    "class MLPClassifier(nn.Module):\n",
    "    '''\n",
    "    A classifier which builds on top of the transformer encoder.\n",
    "    A simple feedforward network with dropout and ReLU activation is used\n",
    "    with the number of layers being a hyperparameter.\n",
    "    '''\n",
    "    def __init__(self, n_features = 256, num_classes = 2, hidden_size = 256, num_layers = 2, dropout=0.2):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(n_features, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #We can choose the number of layers\n",
    "        self.layers = clones(nn.Linear(hidden_size, hidden_size), num_layers-1)\n",
    "        if (num_classes == 2):\n",
    "            self.out = nn.Linear(hidden_size, 1)\n",
    "        else:\n",
    "            self.out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Pass through the classifier\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder.forward(x, mask)\n",
    "\n",
    "        #Take the [CLS] token\n",
    "        x = x[0]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data and putting into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from loader import Lang, ToxicityDataset, normalizeString, collate\n",
    "\n",
    "lang = Lang(\"eng\")\n",
    "data = pd.read_csv('data/train_2024.csv')\n",
    "df = pd.DataFrame(data)\n",
    "for sentence in df['text']:\n",
    "    lang.addSentence(normalizeString(sentence))   \n",
    "\n",
    "trainset = ToxicityDataset('data/train_2024.csv', 'id', 'text', 'label', lang)\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderClassifier(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(70148, 256)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x EncoderBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1028, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=1028, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): MLPClassifier(\n",
       "    (fc): Linear(in_features=256, out_features=1028, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Linear(in_features=1028, out_features=1028, bias=True)\n",
       "    )\n",
       "    (out): Linear(in_features=1028, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 256\n",
    "\n",
    "encoder = Encoder(src_vocab_size=trainset.lang.n_words, n_blocks = 4, n_features = embed_size, n_heads = 4, n_hidden = 512, dropout = 0.1, max_length = 5000)\n",
    "classifier = MLPClassifier(n_features = embed_size, num_classes = 2, hidden_size = 512, num_layers = 4, dropout = 0.2)\n",
    "encoder_classifier = EncoderClassifier(encoder, classifier)\n",
    "\n",
    "encoder.to(device)\n",
    "classifier.to(device)\n",
    "encoder_classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test_2024.csv')\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_lang = Lang(\"eng\")\n",
    "for sentence in test_df['text']:\n",
    "    test_lang.addSentence(normalizeString(sentence))\n",
    "\n",
    "testset = ToxicityDataset('data/test_2024.csv', 'id', 'text', 'label', test_lang)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset), shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, mask, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs, mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 0.6912\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(encoder_classifier.parameters(), lr=0.01)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    encoder_classifier.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, mask, labels = data\n",
    "\n",
    "        # Move tensors to the device\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = torch.tensor([label.item() for label in labels]).unsqueeze(1).to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = encoder_classifier(inputs, mask)\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        # Print batch loss\n",
    "        print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate epoch-level statistics\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = correct / total * 100.0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6622],\n",
       "        [-0.3228],\n",
       "        [ 0.3943]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
