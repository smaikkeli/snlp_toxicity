{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from loader import Lang, ToxicityDataset, normalizeString, collate\n",
    "\n",
    "lang = Lang(\"eng\")\n",
    "data = pd.read_csv('data/train_2024.csv', quoting = 3)\n",
    "df = pd.DataFrame(data)\n",
    "for sentence in df['text']:\n",
    "    lang.addSentence(normalizeString(sentence))   \n",
    "\n",
    "trainset = ToxicityDataset('data/train_2024.csv', 'id', 'text', 'label', lang)\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:\n",
      " as word indices:  tensor([ 21,  22,  23,  24,  25,  26,  15,  27,  28,  29,  31,  32,  33,  34,\n",
      "         35,  24,  36,  15,  37,  38,  39,  40,  34,  35,  41,  39,  43,  44,\n",
      "         41,  45,  46,  47,  48,  49, 413,  50,  48,  51,  35,  52,  25,  53,\n",
      "         54,  58,  59,  60,  34,  29,  30,  61,  36, 413,  52,  62,  38,  63,\n",
      "         64,  48,  65,  50,  48,  66,  67,  69,  70,  21,  63,  68,  71,  72,\n",
      "         28,  73,  74,  75,  76,   3,  24,  77,  79,  68,  80,  81,  82,  28,\n",
      "         83,  84,  85,  36,   3,  12,  88,  48,  90,  93,  25,  94,  95,  95,\n",
      "         63,  68,  34,  98,  38,  63,  99,  28,  73, 102,  52,  62,  60,  65,\n",
      "        103,  38, 104, 106,  28,  42,  38, 102, 104, 107,  15, 108,  49, 109,\n",
      "         50, 110,  49, 111, 112,   1])\n",
      " as string:  i find funny is the loyalty and blindness of english worst possible choice for them is liberal and yet they keep voting for them every keep renewing hope every election year prior to it  just to ignore them at the winning speach have more respect for english community then liberal  at least they dont lie to you just to get your being said i dont vote either tired of those old man but that is another mostly vote local candidate regardless of party even voted liberal that was hard to seriously drop the act anti anti dont vote for cause they dont win of those will at least respect you when they say most of time they will say yes and on it not just saying it like liberals EOS\n"
     ]
    }
   ],
   "source": [
    "#Get random sample from trainset\n",
    "src_seq, label = trainset[1]\n",
    "\n",
    "print('Source sentence:')\n",
    "print(' as word indices: ', src_seq)\n",
    "print(' as string: ', ' '.join(trainset.lang.index2word[i.item()] for i in src_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test_2024.csv', quoting = 3)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_lang = Lang(\"eng\")\n",
    "for sentence in test_df['text']:\n",
    "    test_lang.addSentence(normalizeString(sentence))\n",
    "\n",
    "testset = ToxicityDataset('data/test_2024.csv', 'id', 'text', 'label', test_lang)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset), shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_features, n_heads, n_hidden = 1024, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_features, n_heads, batch_first = True)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_features, n_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x2, _ = self.attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(x2)\n",
    "        x = self.norm1(x)\n",
    "        x2 = self.feed_forward(x)\n",
    "        x = x + self.dropout2(x2)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clones(module, N):\n",
    "    \"Produces N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, n_blocks = 4, n_features = 256, n_heads = 16, n_hidden=512, dropout=0.1, max_length = 5000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, n_features)\n",
    "        self.pos_embedding = nn.Embedding(max_length, n_features)\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(n_features, n_heads, n_hidden, dropout) for _ in range(n_blocks)])\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        B, T = x.size()\n",
    "        positions = torch.arange(0, T, device = device)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_embedding(positions)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier on top of the encoder\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, n_features=512, num_classes=2, num_layers=3, dropout=0.2):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        #A single layer\n",
    "        layers = [\n",
    "            nn.Linear(n_features, n_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        ]\n",
    "        \n",
    "        #Append all layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(n_features * 4, n_features * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "\n",
    "        #The output layer\n",
    "        if num_classes == 2:\n",
    "            layers.append(nn.Linear(n_features * 4, 1))\n",
    "        else:\n",
    "            layers.append(nn.Linear(n_features * 4, num_classes))\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderClassifier(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super(EncoderClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder(x, mask)\n",
    "        \n",
    "        #Take the [CLS] token\n",
    "        #x = x[:,   0, :]\n",
    "\n",
    "        x = x.mean(dim = 1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleEncoderClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512):\n",
    "        super(SimpleEncoderClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(embedding_dim*4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        # Take the [CLS] token\n",
    "        #x = x[:, 0, :]\n",
    "        x = x.mean(dim = 1)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "\n",
    "#This correponds to the first model I tried\n",
    "bert_encoder = Encoder(src_vocab_size=trainset.lang.n_words, n_blocks = 3, n_features = embed_size, n_heads = 4, n_hidden = embed_size, dropout = 0.1, max_length = 5000)\n",
    "classifier = MLPClassifier(n_features = embed_size, num_classes = 2, num_layers = 2, dropout = 0.1)\n",
    "encoder_classifier = EncoderClassifier(bert_encoder, classifier)\n",
    "\n",
    "#This is a simpler one with much less parameters\n",
    "simple_encoder = Encoder(src_vocab_size = trainset.lang.n_words, n_blocks = 3, n_features = 256, n_heads = 4, n_hidden = 256, dropout = 0.1, max_length = 5000)\n",
    "simple_classifier = MLPClassifier(n_features = 256, num_classes = 2, num_layers = 3, dropout = 0.1)\n",
    "encoder_classifier_2 = EncoderClassifier(simple_encoder, simple_classifier)\n",
    "\n",
    "#This is the simplest one, which implements pytorch builtin encoder layers and a simple classifier in one class\n",
    "simple_encoder_classifier = SimpleEncoderClassifier(vocab_size = trainset.lang.n_words, embedding_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7534],\n",
      "        [-0.6633],\n",
      "        [-0.9696],\n",
      "        [-0.6990],\n",
      "        [-0.8221],\n",
      "        [-0.8310],\n",
      "        [-0.7843],\n",
      "        [-0.7840],\n",
      "        [-0.6412],\n",
      "        [-0.8956],\n",
      "        [-0.7508],\n",
      "        [-0.6645],\n",
      "        [-0.8478],\n",
      "        [-0.8355],\n",
      "        [-0.7087],\n",
      "        [-0.9064],\n",
      "        [-0.7706],\n",
      "        [-0.6872],\n",
      "        [-0.8381],\n",
      "        [-0.8109],\n",
      "        [-0.7976],\n",
      "        [-0.7944],\n",
      "        [-0.7779],\n",
      "        [-0.7242],\n",
      "        [-0.8658],\n",
      "        [-0.8156],\n",
      "        [-0.8460],\n",
      "        [-0.9442],\n",
      "        [-0.7968],\n",
      "        [-0.6567],\n",
      "        [-0.8387],\n",
      "        [-0.7515]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 1, Loss: 0.7709\n",
      "tensor([[-0.2834],\n",
      "        [-0.3077],\n",
      "        [-0.3777],\n",
      "        [-0.3622],\n",
      "        [-0.3740],\n",
      "        [-0.3315],\n",
      "        [-0.3218],\n",
      "        [-0.3296],\n",
      "        [-0.3346],\n",
      "        [-0.2713],\n",
      "        [-0.4014],\n",
      "        [-0.1948],\n",
      "        [-0.3335],\n",
      "        [-0.2832],\n",
      "        [-0.3580],\n",
      "        [-0.2810],\n",
      "        [-0.3839],\n",
      "        [-0.1881],\n",
      "        [-0.2616],\n",
      "        [-0.3915],\n",
      "        [-0.3883],\n",
      "        [-0.3338],\n",
      "        [-0.4109],\n",
      "        [-0.3960],\n",
      "        [-0.3028],\n",
      "        [-0.3319],\n",
      "        [-0.2656],\n",
      "        [-0.2831],\n",
      "        [-0.3698],\n",
      "        [-0.3413],\n",
      "        [-0.2800],\n",
      "        [-0.2624]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 2, Loss: 0.6649\n",
      "tensor([[-0.9030],\n",
      "        [-0.7174],\n",
      "        [-0.7789],\n",
      "        [-0.7333],\n",
      "        [-0.6524],\n",
      "        [-0.8183],\n",
      "        [-0.3423],\n",
      "        [-0.7039],\n",
      "        [-0.5471],\n",
      "        [-0.7271],\n",
      "        [-0.5425],\n",
      "        [-0.9552],\n",
      "        [-0.8227],\n",
      "        [-0.8380],\n",
      "        [-0.7610],\n",
      "        [-0.6508],\n",
      "        [-0.7027],\n",
      "        [-0.7869],\n",
      "        [-0.5864],\n",
      "        [-0.3941],\n",
      "        [-0.4060],\n",
      "        [-0.7388],\n",
      "        [-0.8214],\n",
      "        [-0.7696],\n",
      "        [-0.7079],\n",
      "        [-0.7546],\n",
      "        [-0.8152],\n",
      "        [-0.8388],\n",
      "        [-0.7721],\n",
      "        [-0.8033],\n",
      "        [-0.6246],\n",
      "        [-0.3350]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 3, Loss: 0.6673\n",
      "tensor([[ 0.1745],\n",
      "        [-0.0285],\n",
      "        [ 0.0252],\n",
      "        [ 0.2225],\n",
      "        [ 0.1975],\n",
      "        [-0.0610],\n",
      "        [ 0.1117],\n",
      "        [ 0.3088],\n",
      "        [-0.1645],\n",
      "        [ 0.0747],\n",
      "        [ 0.1090],\n",
      "        [-0.0253],\n",
      "        [ 0.2353],\n",
      "        [-0.1194],\n",
      "        [-0.1167],\n",
      "        [-0.0555],\n",
      "        [ 0.0799],\n",
      "        [ 0.0703],\n",
      "        [ 0.0944],\n",
      "        [-0.0098],\n",
      "        [ 0.0984],\n",
      "        [ 0.0511],\n",
      "        [ 0.1217],\n",
      "        [ 0.0427],\n",
      "        [-0.3020],\n",
      "        [ 0.0441],\n",
      "        [ 0.2386],\n",
      "        [ 0.0458],\n",
      "        [ 0.1767],\n",
      "        [ 0.1835],\n",
      "        [ 0.1851],\n",
      "        [ 0.1168]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 4, Loss: 0.7168\n",
      "tensor([[-0.5007],\n",
      "        [-0.5179],\n",
      "        [-0.3833],\n",
      "        [-0.4877],\n",
      "        [-0.4847],\n",
      "        [-0.5377],\n",
      "        [-0.5899],\n",
      "        [-0.5348],\n",
      "        [-0.4254],\n",
      "        [-0.4404],\n",
      "        [-0.3990],\n",
      "        [-0.4928],\n",
      "        [-0.5763],\n",
      "        [-0.3584],\n",
      "        [-0.5482],\n",
      "        [-0.5504],\n",
      "        [-0.5529],\n",
      "        [-0.5145],\n",
      "        [-0.5996],\n",
      "        [-0.4125],\n",
      "        [-0.5233],\n",
      "        [-0.4370],\n",
      "        [-0.5953],\n",
      "        [-0.3890],\n",
      "        [-0.6189],\n",
      "        [-0.5519],\n",
      "        [-0.3067],\n",
      "        [-0.7395],\n",
      "        [-0.6367],\n",
      "        [-0.3281],\n",
      "        [-0.4377],\n",
      "        [-0.6601]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 5, Loss: 0.6773\n",
      "tensor([[-0.3675],\n",
      "        [-0.4276],\n",
      "        [-0.4886],\n",
      "        [-0.4847],\n",
      "        [-0.4397],\n",
      "        [-0.4004],\n",
      "        [-0.4862],\n",
      "        [-0.2748],\n",
      "        [-0.4168],\n",
      "        [-0.3824],\n",
      "        [-0.3354],\n",
      "        [-0.5151],\n",
      "        [-0.4762],\n",
      "        [-0.3026],\n",
      "        [-0.3618],\n",
      "        [-0.4033],\n",
      "        [-0.2579],\n",
      "        [-0.4348],\n",
      "        [-0.3179],\n",
      "        [-0.4032],\n",
      "        [-0.4868],\n",
      "        [-0.5371],\n",
      "        [-0.2755],\n",
      "        [-0.4163],\n",
      "        [-0.3483],\n",
      "        [-0.4075],\n",
      "        [-0.4613],\n",
      "        [-0.5008],\n",
      "        [-0.3897],\n",
      "        [-0.4613],\n",
      "        [-0.2908],\n",
      "        [-0.4919]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 6, Loss: 0.6358\n",
      "tensor([[-1.0232],\n",
      "        [-1.0055],\n",
      "        [-0.8533],\n",
      "        [-1.1966],\n",
      "        [-0.9691],\n",
      "        [-0.9398],\n",
      "        [-1.1103],\n",
      "        [-1.0611],\n",
      "        [-0.6288],\n",
      "        [-0.5430],\n",
      "        [-1.0180],\n",
      "        [-0.8697],\n",
      "        [-0.9875],\n",
      "        [-1.0187],\n",
      "        [-0.7133],\n",
      "        [-0.9831],\n",
      "        [-1.0575],\n",
      "        [-0.8196],\n",
      "        [-1.1206],\n",
      "        [-0.8412],\n",
      "        [-1.0288],\n",
      "        [-0.9301],\n",
      "        [-0.8889],\n",
      "        [-0.9787],\n",
      "        [-0.9502],\n",
      "        [-0.7124],\n",
      "        [-0.9867],\n",
      "        [-0.6865],\n",
      "        [-0.9858],\n",
      "        [-0.9465],\n",
      "        [-0.8602],\n",
      "        [-1.1497]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 7, Loss: 0.7386\n",
      "tensor([[-0.2246],\n",
      "        [-0.5456],\n",
      "        [-0.4299],\n",
      "        [-0.5256],\n",
      "        [-0.3823],\n",
      "        [-0.4985],\n",
      "        [-0.4891],\n",
      "        [-0.5388],\n",
      "        [-0.5745],\n",
      "        [-0.4489],\n",
      "        [-0.4861],\n",
      "        [-0.3042],\n",
      "        [-0.4021],\n",
      "        [-0.4474],\n",
      "        [-0.5966],\n",
      "        [-0.4539],\n",
      "        [-0.5449],\n",
      "        [-0.4776],\n",
      "        [-0.5250],\n",
      "        [-0.5207],\n",
      "        [-0.5074],\n",
      "        [-0.4066],\n",
      "        [-0.4918],\n",
      "        [-0.4891],\n",
      "        [-0.4819],\n",
      "        [-0.4358],\n",
      "        [-0.5316],\n",
      "        [-0.3865],\n",
      "        [-0.4474],\n",
      "        [-0.5561],\n",
      "        [-0.5265],\n",
      "        [-0.6270]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 8, Loss: 0.6689\n",
      "tensor([[-0.0822],\n",
      "        [-0.0796],\n",
      "        [-0.1517],\n",
      "        [-0.2820],\n",
      "        [-0.1814],\n",
      "        [-0.2229],\n",
      "        [-0.1225],\n",
      "        [-0.2008],\n",
      "        [-0.0688],\n",
      "        [-0.1034],\n",
      "        [-0.2099],\n",
      "        [-0.2042],\n",
      "        [-0.1895],\n",
      "        [-0.1570],\n",
      "        [-0.2437],\n",
      "        [-0.2774],\n",
      "        [-0.1500],\n",
      "        [-0.2113],\n",
      "        [-0.3129],\n",
      "        [-0.2633],\n",
      "        [-0.2174],\n",
      "        [-0.1618],\n",
      "        [-0.1715],\n",
      "        [-0.1997],\n",
      "        [-0.2211],\n",
      "        [-0.0514],\n",
      "        [-0.0691],\n",
      "        [-0.2505],\n",
      "        [-0.0971],\n",
      "        [-0.0766],\n",
      "        [-0.1839],\n",
      "        [-0.1916]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 9, Loss: 0.7070\n",
      "tensor([[ 0.1360],\n",
      "        [-0.0309],\n",
      "        [ 0.0655],\n",
      "        [ 0.1394],\n",
      "        [-0.0089],\n",
      "        [ 0.1305],\n",
      "        [ 0.0611],\n",
      "        [ 0.1521],\n",
      "        [ 0.1373],\n",
      "        [ 0.1438],\n",
      "        [ 0.1955],\n",
      "        [ 0.0110],\n",
      "        [-0.0097],\n",
      "        [ 0.2105],\n",
      "        [ 0.0289],\n",
      "        [ 0.0503],\n",
      "        [ 0.0727],\n",
      "        [ 0.1019],\n",
      "        [ 0.1299],\n",
      "        [ 0.0129],\n",
      "        [ 0.1837],\n",
      "        [ 0.0138],\n",
      "        [ 0.0574],\n",
      "        [ 0.1395],\n",
      "        [ 0.2043],\n",
      "        [ 0.0156],\n",
      "        [ 0.0978],\n",
      "        [ 0.0702],\n",
      "        [ 0.1074],\n",
      "        [ 0.0511],\n",
      "        [ 0.1232],\n",
      "        [ 0.1913]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 10, Loss: 0.6947\n",
      "tensor([[0.2465],\n",
      "        [0.2220],\n",
      "        [0.3285],\n",
      "        [0.3743],\n",
      "        [0.1323],\n",
      "        [0.2165],\n",
      "        [0.2853],\n",
      "        [0.1753],\n",
      "        [0.1861],\n",
      "        [0.2529],\n",
      "        [0.3356],\n",
      "        [0.3522],\n",
      "        [0.2024],\n",
      "        [0.3357],\n",
      "        [0.3650],\n",
      "        [0.3980],\n",
      "        [0.1757],\n",
      "        [0.3559],\n",
      "        [0.3009],\n",
      "        [0.3737],\n",
      "        [0.2654],\n",
      "        [0.3166],\n",
      "        [0.1741],\n",
      "        [0.2423],\n",
      "        [0.2638],\n",
      "        [0.2336],\n",
      "        [0.0853],\n",
      "        [0.0631],\n",
      "        [0.0865],\n",
      "        [0.2979],\n",
      "        [0.3514],\n",
      "        [0.2647]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 11, Loss: 0.7475\n",
      "tensor([[-0.0386],\n",
      "        [-0.0454],\n",
      "        [-0.0035],\n",
      "        [ 0.0830],\n",
      "        [ 0.0163],\n",
      "        [ 0.0136],\n",
      "        [ 0.0368],\n",
      "        [-0.0022],\n",
      "        [ 0.0133],\n",
      "        [ 0.0253],\n",
      "        [ 0.0429],\n",
      "        [ 0.0666],\n",
      "        [ 0.0117],\n",
      "        [ 0.0202],\n",
      "        [ 0.0489],\n",
      "        [ 0.0491],\n",
      "        [ 0.0232],\n",
      "        [-0.0231],\n",
      "        [-0.0111],\n",
      "        [ 0.0726],\n",
      "        [ 0.0243],\n",
      "        [ 0.0471],\n",
      "        [ 0.0253],\n",
      "        [-0.0150],\n",
      "        [-0.0189],\n",
      "        [ 0.0972],\n",
      "        [ 0.0998],\n",
      "        [ 0.0130],\n",
      "        [ 0.0855],\n",
      "        [-0.0023],\n",
      "        [-0.0297],\n",
      "        [-0.0499]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 12, Loss: 0.6926\n",
      "tensor([[-0.0819],\n",
      "        [-0.1924],\n",
      "        [-0.1711],\n",
      "        [-0.1800],\n",
      "        [-0.2375],\n",
      "        [-0.2174],\n",
      "        [-0.1823],\n",
      "        [-0.1699],\n",
      "        [-0.1624],\n",
      "        [-0.1617],\n",
      "        [-0.2130],\n",
      "        [-0.1095],\n",
      "        [-0.1614],\n",
      "        [-0.1869],\n",
      "        [-0.1067],\n",
      "        [-0.1234],\n",
      "        [-0.1827],\n",
      "        [-0.1884],\n",
      "        [-0.1916],\n",
      "        [-0.0634],\n",
      "        [-0.0865],\n",
      "        [-0.1643],\n",
      "        [-0.1762],\n",
      "        [-0.0917],\n",
      "        [-0.1694],\n",
      "        [-0.1487],\n",
      "        [-0.1029],\n",
      "        [-0.2025],\n",
      "        [-0.1898],\n",
      "        [-0.2386],\n",
      "        [-0.1714],\n",
      "        [-0.0867]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 13, Loss: 0.6678\n",
      "tensor([[-0.4229],\n",
      "        [-0.2682],\n",
      "        [-0.4411],\n",
      "        [-0.4483],\n",
      "        [-0.3547],\n",
      "        [-0.3972],\n",
      "        [-0.3850],\n",
      "        [-0.3804],\n",
      "        [-0.4674],\n",
      "        [-0.2772],\n",
      "        [-0.3478],\n",
      "        [-0.2871],\n",
      "        [-0.3847],\n",
      "        [-0.3638],\n",
      "        [-0.4574],\n",
      "        [-0.3451],\n",
      "        [-0.2855],\n",
      "        [-0.4651],\n",
      "        [-0.3662],\n",
      "        [-0.3849],\n",
      "        [-0.2954],\n",
      "        [-0.4594],\n",
      "        [-0.2782],\n",
      "        [-0.3510],\n",
      "        [-0.3016],\n",
      "        [-0.3986],\n",
      "        [-0.4326],\n",
      "        [-0.2782],\n",
      "        [-0.4808],\n",
      "        [-0.4016],\n",
      "        [-0.3910],\n",
      "        [-0.4215]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 14, Loss: 0.6516\n",
      "tensor([[-0.7316],\n",
      "        [-0.4762],\n",
      "        [-0.6425],\n",
      "        [-0.7402],\n",
      "        [-0.4634],\n",
      "        [-0.6419],\n",
      "        [-0.6667],\n",
      "        [-0.6272],\n",
      "        [-0.7434],\n",
      "        [-0.5512],\n",
      "        [-0.6056],\n",
      "        [-0.6354],\n",
      "        [-0.7953],\n",
      "        [-0.5027],\n",
      "        [-0.6127],\n",
      "        [-0.6259],\n",
      "        [-0.4920],\n",
      "        [-0.4569],\n",
      "        [-0.6429],\n",
      "        [-0.6438],\n",
      "        [-0.6757],\n",
      "        [-0.7165],\n",
      "        [-0.4611],\n",
      "        [-0.5164],\n",
      "        [-0.8035],\n",
      "        [-0.7051],\n",
      "        [-0.5035],\n",
      "        [-0.5496],\n",
      "        [-0.6495],\n",
      "        [-0.6485],\n",
      "        [-0.6873],\n",
      "        [-0.5644]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 15, Loss: 0.5930\n",
      "tensor([[-1.2640],\n",
      "        [-1.0375],\n",
      "        [-1.3189],\n",
      "        [-1.1145],\n",
      "        [-1.0263],\n",
      "        [-0.9159],\n",
      "        [-1.0711],\n",
      "        [-0.8163],\n",
      "        [-0.9492],\n",
      "        [-1.2333],\n",
      "        [-0.9486],\n",
      "        [-0.7186],\n",
      "        [-1.2868],\n",
      "        [-0.8776],\n",
      "        [-0.7513],\n",
      "        [-0.7750],\n",
      "        [-1.3921],\n",
      "        [-0.8644],\n",
      "        [-1.3008],\n",
      "        [-1.2969],\n",
      "        [-1.2076],\n",
      "        [-1.0663],\n",
      "        [-1.1795],\n",
      "        [-1.2582],\n",
      "        [-0.9361],\n",
      "        [-1.2910],\n",
      "        [-1.3161],\n",
      "        [-0.8588],\n",
      "        [-1.0790],\n",
      "        [-1.0895],\n",
      "        [-1.2668],\n",
      "        [-1.1271]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 16, Loss: 0.7205\n",
      "tensor([[-0.6658],\n",
      "        [-0.6574],\n",
      "        [-0.9902],\n",
      "        [-0.8516],\n",
      "        [-1.0042],\n",
      "        [-0.9512],\n",
      "        [-0.9958],\n",
      "        [-0.9995],\n",
      "        [-0.8609],\n",
      "        [-1.0812],\n",
      "        [-0.7602],\n",
      "        [-1.0339],\n",
      "        [-0.9778],\n",
      "        [-1.1571],\n",
      "        [-1.0614],\n",
      "        [-0.7630],\n",
      "        [-1.0149],\n",
      "        [-1.0011],\n",
      "        [-1.0199],\n",
      "        [-1.0089],\n",
      "        [-1.0691],\n",
      "        [-1.2037],\n",
      "        [-0.9226],\n",
      "        [-0.9818],\n",
      "        [-1.0432],\n",
      "        [-0.8813],\n",
      "        [-0.7348],\n",
      "        [-1.0212],\n",
      "        [-0.9175],\n",
      "        [-1.2418],\n",
      "        [-1.0341],\n",
      "        [-1.0530]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 17, Loss: 0.5775\n",
      "tensor([[-0.9721],\n",
      "        [-0.8376],\n",
      "        [-1.1022],\n",
      "        [-0.9015],\n",
      "        [-0.8396],\n",
      "        [-0.8608],\n",
      "        [-0.7341],\n",
      "        [-0.9025],\n",
      "        [-0.9825],\n",
      "        [-0.7604],\n",
      "        [-0.7756],\n",
      "        [-0.7022],\n",
      "        [-1.0729],\n",
      "        [-0.9801],\n",
      "        [-0.9808],\n",
      "        [-0.8251],\n",
      "        [-0.8056],\n",
      "        [-0.8548],\n",
      "        [-0.7719],\n",
      "        [-0.9664],\n",
      "        [-0.8553],\n",
      "        [-0.6119],\n",
      "        [-0.8609],\n",
      "        [-0.8372],\n",
      "        [-1.0176],\n",
      "        [-0.9959],\n",
      "        [-0.8383],\n",
      "        [-0.9853],\n",
      "        [-1.0129],\n",
      "        [-0.7310],\n",
      "        [-1.1486],\n",
      "        [-0.9267]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 18, Loss: 0.6770\n",
      "tensor([[-0.4463],\n",
      "        [-0.6441],\n",
      "        [-0.7201],\n",
      "        [-0.6904],\n",
      "        [-0.4648],\n",
      "        [-0.5054],\n",
      "        [-0.6925],\n",
      "        [-0.6695],\n",
      "        [-0.6791],\n",
      "        [-0.6007],\n",
      "        [-0.6331],\n",
      "        [-0.7258],\n",
      "        [-0.6970],\n",
      "        [-0.6500],\n",
      "        [-0.4980],\n",
      "        [-0.6453],\n",
      "        [-0.6052],\n",
      "        [-0.5576],\n",
      "        [-0.5295],\n",
      "        [-0.5380],\n",
      "        [-0.4870],\n",
      "        [-0.4001],\n",
      "        [-0.5954],\n",
      "        [-0.6151],\n",
      "        [-0.5478],\n",
      "        [-0.8041],\n",
      "        [-0.7335],\n",
      "        [-0.6871],\n",
      "        [-0.6290],\n",
      "        [-0.5579],\n",
      "        [-0.7998],\n",
      "        [-0.6643]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 19, Loss: 0.7875\n",
      "tensor([[-0.3313],\n",
      "        [-0.3101],\n",
      "        [-0.2913],\n",
      "        [-0.2426],\n",
      "        [-0.3048],\n",
      "        [-0.2681],\n",
      "        [-0.3118],\n",
      "        [-0.3241],\n",
      "        [-0.2026],\n",
      "        [-0.3398],\n",
      "        [-0.3025],\n",
      "        [-0.3826],\n",
      "        [-0.3119],\n",
      "        [-0.2631],\n",
      "        [-0.2889],\n",
      "        [-0.3227],\n",
      "        [-0.3025],\n",
      "        [-0.2955],\n",
      "        [-0.2812],\n",
      "        [-0.2936],\n",
      "        [-0.2680],\n",
      "        [-0.2525],\n",
      "        [-0.3245],\n",
      "        [-0.2761],\n",
      "        [-0.2969],\n",
      "        [-0.1946],\n",
      "        [-0.3175],\n",
      "        [-0.3399],\n",
      "        [-0.3395],\n",
      "        [-0.2945],\n",
      "        [-0.3085],\n",
      "        [-0.3335]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 1, Batch 20, Loss: 0.6558\n",
      "tensor([[-0.2214],\n",
      "        [-0.1942],\n",
      "        [-0.1894],\n",
      "        [-0.1958],\n",
      "        [-0.1803],\n",
      "        [-0.2013],\n",
      "        [-0.1975],\n",
      "        [-0.1933],\n",
      "        [-0.2169],\n",
      "        [-0.2037],\n",
      "        [-0.1876],\n",
      "        [-0.1939],\n",
      "        [-0.1925],\n",
      "        [-0.2138],\n",
      "        [-0.1903],\n",
      "        [-0.2303],\n",
      "        [-0.1921],\n",
      "        [-0.2323],\n",
      "        [-0.1882],\n",
      "        [-0.1876],\n",
      "        [-0.2077],\n",
      "        [-0.2253],\n",
      "        [-0.2125],\n",
      "        [-0.1940],\n",
      "        [-0.2091],\n",
      "        [-0.2183],\n",
      "        [-0.2065],\n",
      "        [-0.2100],\n",
      "        [-0.2419],\n",
      "        [-0.2097],\n",
      "        [-0.1987],\n",
      "        [-0.1649]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m---> 35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Compute total loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = encoder_classifier\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas = (0.9, 0.98), eps=1e-9)\n",
    "\n",
    "#Calculate the positive weight fraction\n",
    "positive  = sum([label for _, label in trainset])\n",
    "negative = len(trainset) - positive\n",
    "positive_weight = negative/positive\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs, mask, labels = data\n",
    "    \n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(torch.float32).reshape(labels.size(0), 1).to(device)\n",
    "        \n",
    "        outputs = model(inputs, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        total = labels.size(0)\n",
    "\n",
    "        # Print batch loss\n",
    "        #if i % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss += total_loss / len(train_loader)\n",
    "        epoch_accuracy += correct / total * 100.0\n",
    "\n",
    "    # Calculate epoch-level statistics\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = correct / total * 100.0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_accuracy\u001b[39m(model, data_loader):\n",
      "\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;32m      3\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n",
      "\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n",
      "\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n",
      "\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_wait_suspend(thread, frame, event, arg)\n",
      "\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n",
      "\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdo_wait_suspend(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n",
      "\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n",
      "\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n",
      "\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n",
      "\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/snlp/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n",
      "\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n",
      "\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n",
      "\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n",
      "\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n",
      "\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, mask, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        mask = mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs, mask)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
